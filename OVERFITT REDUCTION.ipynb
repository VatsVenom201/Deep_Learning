{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed73e573-92a5-4348-b686-4330c12aa4b4",
   "metadata": {},
   "source": [
    "# üîπ Technique 1: Data Augmentation (Primary way to reduce overfitting)\n",
    "### What problem it solves\n",
    "\n",
    "Overfitting happens when:\n",
    "\n",
    "CNN memorizes training images\n",
    "\n",
    "Model performs well on train data but poorly on validation data\n",
    "\n",
    "#### Data augmentation increases data diversity without collecting new data.\n",
    "\n",
    "### Core idea\n",
    "\n",
    "#### Artificially modify training images so the model sees different versions of the same image.\n",
    "\n",
    "This forces the CNN to learn general features, not exact pixel patterns. \n",
    "##### ONLY on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a0cd5-e890-4758-a85d-8899b9bea992",
   "metadata": {},
   "source": [
    "## Method 1: Using Keras preprocessing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab850b-1996-4ee6-8b66-984477080df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),    # Flips image left ‚Üî right\n",
    "    tf.keras.layers.RandomRotation(0.1),         # Rotates image randomly up to ¬±10%\n",
    "    tf.keras.layers.RandomZoom(0.1),             # Randomly zooms in/out\n",
    "    tf.keras.layers.RandomTranslation(0.1, 0.1)  # Shifts image horizontally and vertically\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0b338-16c0-4ec3-afd5-7c60d5dbba0a",
   "metadata": {},
   "source": [
    "How to use it in a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3f1da-c33f-47e0-bdce-b233652fc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(224, 224, 3)),\n",
    "\n",
    "    data_augmentation,          # üëà augmentation applied here\n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad999bf-195d-49d0-afbd-eff7bb9c9a26",
   "metadata": {},
   "source": [
    "### Data augmentation reduces overfitting by artificially increasing training data diversity, forcing the CNN to learn invariant and generalizable features instead of memorizing exact patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d9aac-ba97-4433-8548-37418b8b7163",
   "metadata": {},
   "source": [
    "# üîπ Technique 2: Early Stopping\n",
    "#### What problem it solves\n",
    "\n",
    "Overfitting often happens when:\n",
    "\n",
    "Training loss keeps decreasing\n",
    "\n",
    "Validation loss starts increasing\n",
    "\n",
    "This means the model has started memorizing training data.\n",
    "\n",
    "#### Early Stopping stops training at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99041f-9068-42a9-8487-84aca8aa6f86",
   "metadata": {},
   "source": [
    "#### Core idea - Stop training when validation performance stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12782b-ff0a-48a6-9cbc-f6d0e2a38464",
   "metadata": {},
   "source": [
    "As a callback, Not inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75c707-f967-4ada-9a11-235cc23ca17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # Metric to observe\n",
    "    patience=5,               # Number of epochs to wait after no improvement\n",
    "    restore_best_weights=True # Restores weights from the epoch with lowest validation loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34193e2d-245f-4095-8e42-e8e17bd8c93c",
   "metadata": {},
   "source": [
    "How to use it in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb8b0d-5e4b-448a-835e-c5670405ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ee278-9358-4a58-8b00-9e9bb6b388b3",
   "metadata": {},
   "source": [
    "# üîπ Technique 3: Dropout Regularization\n",
    "#### What problem it solves\n",
    "\n",
    "Overfitting happens when:\n",
    "\n",
    "Neurons rely too much on specific other neurons\n",
    "\n",
    "The network memorizes patterns instead of generalizing\n",
    "\n",
    "Dropout breaks these dependencies.\n",
    "### Core idea - Randomly deactivate a fraction of neurons during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ae60c-8545-4428-8dc7-9d887d05430d",
   "metadata": {},
   "source": [
    "This forces the network to:\n",
    "\n",
    "Learn redundant representations\n",
    "\n",
    "Become more robust\n",
    "\n",
    "Generalize better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e30ab5-b4d0-43f3-802a-3e48c62d1511",
   "metadata": {},
   "source": [
    "Where Dropout is applied\n",
    "\n",
    "- After Conv layers\n",
    "\n",
    "- After Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf5632-86c6-43b5-bf4c-2142af66d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.25),   # 25% neurons dropped\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba97e3-7883-483b-ae38-1910fb7fce7f",
   "metadata": {},
   "source": [
    "# üîπ Technique 4: L2 Regularization (Weight Decay)\n",
    "What problem it solves\n",
    "\n",
    "Overfitting occurs when:\n",
    "\n",
    "Model learns very large weights\n",
    "\n",
    "Decision boundary becomes overly complex\n",
    "\n",
    "Small noise causes large output changes\n",
    "\n",
    "#### L2 regularization penalizes large weights.\n",
    "### Core idea - Add a penalty proportional to the square of the weights to the loss function\n",
    "\n",
    "Total Loss=Data Loss+Œª‚àëw^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e46c4-407f-4fc3-9e3e-8893bbb62b90",
   "metadata": {},
   "source": [
    "#### Where L2 regularization is applied\n",
    "\n",
    "‚úÖ On Conv2D layers\n",
    "‚úÖ On Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5b3d9-da3b-4329-aed5-711ffb8a2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3,3),\n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4)  # Strength of regularization(lamda) - 0.0001 ,Standard (most used)\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(\n",
    "        64, (3,3),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(\n",
    "        128,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75378b-a551-434c-b269-ac5e9df3f64d",
   "metadata": {},
   "source": [
    "#### Why L2 reduces overfitting\n",
    "\n",
    "Shrinks large weights\n",
    "\n",
    "Encourages smoother decision boundaries\n",
    "\n",
    "Makes model less sensitive to noise\n",
    "\n",
    "#### Statistically: Reduces variance without increasing bias too much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10ce43-03d1-4d80-8433-814121a49f96",
   "metadata": {},
   "source": [
    "| L2                 | L1                      |\n",
    "| ------------------ | ----------------------- |\n",
    "| Shrinks weights    | Makes some weights zero |\n",
    "| Smooth solution    | Sparse solution         |\n",
    "| Preferred for CNNs | Rare in CNNs            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ff7e0-e3b7-4f05-ac9f-604730ff39f2",
   "metadata": {},
   "source": [
    "### L2 regularization reduces overfitting by penalizing large weights, encouraging smoother and more generalizable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d7fa1-c9a3-4640-b56f-af37db460fce",
   "metadata": {},
   "source": [
    "# üîπ Technique 5: Reduce Model Capacity (Simpler Architecture)\n",
    "What problem it solves\n",
    "\n",
    "Overfitting occurs when:\n",
    "\n",
    "Model is too powerful\n",
    "\n",
    "Dataset is small\n",
    "\n",
    "Model memorizes training samples\n",
    "\n",
    "#### A simpler model generalizes better.\n",
    "\n",
    "## Methods:\n",
    "1) Reduce no. of neurons (units) in dense layers\n",
    "2) Reduce no. of kernels in Conv layers\n",
    "3) Reduce kernel size, ie. Dimension (5,5) -> (3,3)\n",
    "4) Replace Flatten layer to GlobalAveragePooling layer [while switching from conv layers to dense layers]\n",
    "5) Reduce no. of layers (dense/conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682c92a-77af-4c60-9a06-c10984d45685",
   "metadata": {},
   "source": [
    "### Why this reduces overfitting\n",
    "\n",
    "Fewer parameters ‚Üí less memorization\n",
    "\n",
    "Forces model to learn essential features\n",
    "\n",
    "Reduces variance\n",
    "\n",
    "When this technique is MOST useful\n",
    "\n",
    "Small datasets\n",
    "\n",
    "Training from scratch\n",
    "\n",
    "Limited hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fceddf-ecb1-457e-9434-4f2fee2af4b5",
   "metadata": {},
   "source": [
    "### Reducing model capacity lowers overfitting by limiting the number of parameters, preventing the model from memorizing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d301c-02ec-45e4-9d8b-d33a5b176c89",
   "metadata": {},
   "source": [
    "# üîπ Technique 6: Batch Normalization\n",
    "What problem it solves\n",
    "\n",
    "During training:\n",
    "\n",
    "Activations shift as weights update\n",
    "\n",
    "Training becomes unstable\n",
    "\n",
    "Model may overfit and converge poorly\n",
    "\n",
    "#### Batch Normalization stabilizes learning and adds mild regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01650a7-d57f-4af0-9d92-ede64eec3f6d",
   "metadata": {},
   "source": [
    "#### Core idea - Normalize activations within a mini-batch to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c48864-2703-4dc0-953e-8c0bd75f3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic syntax\n",
    "tf.keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d121dc0-8188-4168-9a1a-6637805b56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e8693-97cb-4d5c-afc6-51decc4aeac9",
   "metadata": {},
   "source": [
    "### Why Batch Normalization reduces overfitting\n",
    "\n",
    "Adds noise via batch statistics\n",
    "\n",
    "Reduces sensitivity to initialization\n",
    "\n",
    "Acts as a weak regularizer\n",
    "\n",
    "Improves generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752208f-cfb6-49f5-ac07-3c095b538df5",
   "metadata": {},
   "source": [
    "| BatchNorm              | Dropout                  |\n",
    "| ---------------------- | ------------------------ |\n",
    "| Normalizes activations | Randomly removes neurons |\n",
    "| Stabilizes training    | Prevents co-adaptation   |\n",
    "| Mild regularization    | Strong regularization    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6776587-6164-4909-bedb-5068471f5e48",
   "metadata": {},
   "source": [
    "# üîπ Technique 7: Learning Rate Reduction (ReduceLROnPlateau)\n",
    "What problem it solves\n",
    "\n",
    "Overfitting and poor generalization occur when:\n",
    "\n",
    "Learning rate is too high\n",
    "\n",
    "Model keeps making large weight updates\n",
    "\n",
    "Validation loss stops improving but training continues\n",
    "\n",
    "#### Reducing learning rate allows finer, more generalizable learning.\n",
    "\n",
    "#### Core idea : Automatically reduce the learning rate when validation performance plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a6202-e736-4331-a81a-5a94110869ae",
   "metadata": {},
   "source": [
    "Applied as a callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49091891-8eb2-4cb9-a2d3-0dfa820d1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,             # Multiplies current LR by this factor , Example: 1e-4 ‚Üí 3e-5\n",
    "    patience=3, \n",
    "    min_lr=1e-6             # Lower bound for learning rate , Prevents LR from becoming uselessly small\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e95c1f-4727-41b0-9970-77af61fa674b",
   "metadata": {},
   "source": [
    "How to use it during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb5d85-e6d3-4671-ae06-c038e995bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[lr_reduce]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d9947-fbd6-43da-8b46-983601d9afca",
   "metadata": {},
   "source": [
    "### Learning rate reduction mitigates overfitting by slowing weight updates when validation loss plateaus, enabling more stable and generalizable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee2aa-f4c9-43c5-a89d-e30b9863da45",
   "metadata": {},
   "source": [
    "# üîπ Technique 8: Transfer Learning & Freezing Layers\n",
    "What problem it solves\n",
    "\n",
    "Overfitting happens when:\n",
    "\n",
    "Dataset is small\n",
    "\n",
    "CNN has too many parameters\n",
    "\n",
    "Model learns noise instead of general patterns\n",
    "\n",
    "Transfer learning reduces overfitting by reusing pretrained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c76c98-8e69-4ee5-9eba-e4164ba9d318",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Freezing layers\n",
    "What it means\n",
    "\n",
    "Prevents weight updates in pretrained layers\n",
    "\n",
    "Reduces number of trainable parameters\n",
    "\n",
    "##### Why it helps\n",
    "\n",
    "Prevents overfitting on small datasets\n",
    "\n",
    "Keeps learned generic features intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89173303-d8e0-4a88-8e9c-23b7d6ca88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,             # doesn't have the dense layers or whatever layer at the top\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False       # makes loaded model untrainable, layers freezed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5ba99-5313-40c0-8baf-f520ba65e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([     # adding custom head on the base model\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663324d1-370f-4e73-9fe4-0a76ef82c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6aaed-0c5f-4f94-b4b6-bbafc06af83b",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£Fine-tuning\n",
    "##### What it means\n",
    "\n",
    "Unfreeze top layers of base model\n",
    "\n",
    "Train with very small learning rate\n",
    "\n",
    "##### Why it helps\n",
    "\n",
    "Adapts pretrained features to your dataset\n",
    "\n",
    "Still avoids overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5bc43-1dfb-4f89-87d6-df28c6e4144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True          # unfreezes all the layers\n",
    "\n",
    "for layer in base_model.layers[:-30]:     # makes the initial 30 layers untrainable, ie. freeze\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917a398-b2f5-4fd8-96bb-f897e6477676",
   "metadata": {},
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098682b-d12f-45c8-b5c6-189c66d96d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-30:]:     # makes the last 30 layers untrainable, ie. freeze\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5570f-1ce1-47c9-a675-eb26e6d2a44e",
   "metadata": {},
   "source": [
    "if we have conv1,2,3,....250 layers, lets say we do [-30:] meaning last 30, ie. 220-250\n",
    "\n",
    "if we do [:-30] initial 30, 1-30 layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd194683-3239-4116-9081-a9170b3d0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile with low LR\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcdd006-c353-432b-8620-ea10677d40d3",
   "metadata": {},
   "source": [
    "### Transfer learning mitigates overfitting by leveraging pretrained features and limiting the number of trainable parameters through layer freezing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a8501-7fe0-44cc-813d-9a7a2e1c7e86",
   "metadata": {},
   "source": [
    "# üîπ Technique 9: Early Stopping\n",
    "What problem it solves\n",
    "\n",
    "Overfitting happens when:\n",
    "\n",
    "Training loss keeps decreasing\n",
    "\n",
    "Validation loss starts increasing\n",
    "\n",
    "üëâ Model is memorizing training data.\n",
    "\n",
    "### Early Stopping stops training at the optimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd0dc1-4da8-45a7-963d-09ab3adacc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',       # loss is monitored, we can also monitor val_accuracy \n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8196287-cb14-449d-8861-5dad6b4c0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdeb176-43a1-44df-abec-73b8f98924d7",
   "metadata": {},
   "source": [
    "# üîπ Technique 10: Reduce Learning Rate on Plateau\n",
    "What problem it solves\n",
    "\n",
    "During training:\n",
    "\n",
    "Loss stops improving (plateau)\n",
    "\n",
    "High learning rate prevents finer weight updates\n",
    "\n",
    "Model starts oscillating or overfitting noise\n",
    "\n",
    "###  Lowering LR at the right time allows better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e450e-0454-405b-89ab-6d5e5f220e7e",
   "metadata": {},
   "source": [
    "### Core idea : Automatically reduce the learning rate when validation performance stops improving.\n",
    "\n",
    "Unlike fixed LR schedules, this is adaptive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a05c9-5957-46bb-82cb-b568639158d7",
   "metadata": {},
   "source": [
    "#### Where it is used\n",
    "\n",
    "‚úÖ Implemented as a callback\n",
    "‚úÖ Passed into model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2061035-8045-4e2c-9bce-761d1ad6b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca09c3-571a-4881-bf33-49cf0fa98c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(      # define the callback\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833928f4-ce7c-4349-b187-8e481cdb3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[lr_reduce]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64198ab3-a81e-4642-8c45-f88f7f2e2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## technique 9 & 10 togather..\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a9cc3-b072-47ef-9a2a-1b9729e38617",
   "metadata": {},
   "source": [
    "# Master Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846a1c9-d8a5-4a4d-bdd5-27465b748fcd",
   "metadata": {},
   "source": [
    "| #      | Technique                        | Where Applied      | What It Does                               | Why It Works              | Key Code / Keyword             |\n",
    "| ------ | -------------------------------- | ------------------ | ------------------------------------------ | ------------------------- | ------------------------------ |\n",
    "| **1**  | Data Augmentation                | Input data         | Creates modified versions of training data | Increases data diversity  | `RandomFlip`, `RandomRotation` |\n",
    "| **2**  | Dropout                          | Hidden layers      | Randomly disables neurons                  | Prevents co-adaptation    | `Dropout(0.3‚Äì0.5)`             |\n",
    "| **3**  | L2 Regularization (Weight Decay) | Kernel weights     | Penalizes large weights                    | Encourages simpler models | `kernel_regularizer=l2(1e-4)`  |\n",
    "| **4**  | L1 Regularization                | Kernel weights     | Shrinks some weights to zero               | Feature selection         | `l1(1e-4)`                     |\n",
    "| **5**  | Batch Normalization              | After conv / dense | Normalizes activations                     | Stabilizes learning       | `BatchNormalization()`         |\n",
    "| **6**  | Model Capacity Reduction         | Architecture       | Reduces parameters                         | Limits memorization       | Fewer layers / filters         |\n",
    "| **7**  | Learning Rate Control            | Optimizer          | Slows weight updates                       | Avoids fitting noise      | `Adam(lr=1e-4)`                |\n",
    "| **8**  | Transfer Learning                | Pretrained CNN     | Reuses learned features                    | Fewer trainable params    | `base_model.trainable=False`   |\n",
    "| **9**  | Early Stopping                   | Training loop      | Stops training early                       | Prevents memorization     | `EarlyStopping()`              |\n",
    "| **10** | Reduce LR on Plateau             | Callback           | Lowers LR on stagnation                    | Refines convergence       | `ReduceLROnPlateau()`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135f566-9139-4db1-9aeb-4f05c2d61751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "conda-env-anaconda-ml-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
