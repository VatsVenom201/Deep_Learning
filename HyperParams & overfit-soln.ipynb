{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b15f35-d476-47bc-98c2-e23b76ac6535",
   "metadata": {},
   "source": [
    "## Hyper Parameters of a typical Neural Net...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61513c1-8333-480a-a768-ff1f520ed4e9",
   "metadata": {},
   "source": [
    "What is a hyperparameter (one-line)\n",
    "\n",
    "Hyperparameters are externally set configuration values that govern the training process and network architecture.\n",
    "\n",
    "### 1. Training hyperparameters (most important)\n",
    "\n",
    "These directly affect learning dynamics.\n",
    "\n",
    "#### 1. Learning rate (Î·)\n",
    "\n",
    "Step size for weight updates\n",
    "\n",
    "Too high â†’ divergence\n",
    "\n",
    "Too low â†’ slow learning\n",
    "\n",
    "Typical values: 0.1, 0.01, 0.001\n",
    "\n",
    "#### 2. Batch size\n",
    "\n",
    "Number of samples per gradient update\n",
    "\n",
    "Common values: 16, 32, 64, 128\n",
    "\n",
    "Affects speed, memory, and generalization\n",
    "\n",
    "#### 3. Number of epochs\n",
    "\n",
    "Number of full passes over the dataset\n",
    "\n",
    "Too few â†’ underfitting\n",
    "\n",
    "Too many â†’ overfitting\n",
    "\n",
    "#### 4. Optimizer\n",
    "\n",
    "Algorithm for updating weights\n",
    "\n",
    "Examples:\n",
    "\n",
    "SGD\n",
    "\n",
    "SGD + Momentum\n",
    "\n",
    "Adam (most common)\n",
    "\n",
    "RMSprop\n",
    "\n",
    "#### 5. Momentum (if used)\n",
    "\n",
    "Controls how much past gradients influence current update\n",
    "\n",
    "Typical: 0.9\n",
    "\n",
    "### 2. Architecture hyperparameters\n",
    "\n",
    "These define the structure of the network.\n",
    "\n",
    "#### 6. Number of layers (depth)\n",
    "\n",
    "How many hidden layers\n",
    "\n",
    "More layers â†’ more expressive power\n",
    "\n",
    "Risk of overfitting if excessive\n",
    "\n",
    "#### 7. Number of neurons per layer (width)\n",
    "\n",
    "Controls model capacity\n",
    "\n",
    "Example: 128 â†’ 64 â†’ 32\n",
    "\n",
    "#### 8. Activation function\n",
    "\n",
    "Adds non-linearity\n",
    "\n",
    "Common:\n",
    "\n",
    "ReLU (hidden layers)\n",
    "\n",
    "Sigmoid / Softmax (output layer)\n",
    "\n",
    "#### 9. Weight initialization\n",
    "\n",
    "How initial weights are set\n",
    "\n",
    "Examples:\n",
    "\n",
    "Xavier (for Tanh)\n",
    "\n",
    "He initialization (for ReLU)\n",
    "\n",
    "### 3. Regularization hyperparameters\n",
    "\n",
    "Used to prevent overfitting.\n",
    "\n",
    "#### 10. L2 / L1 regularization (weight decay)\n",
    "\n",
    "Penalizes large weights\n",
    "\n",
    "Common L2 values: 0.0001, 0.001\n",
    "\n",
    "#### 11. Dropout rate\n",
    "\n",
    "Fraction of neurons randomly dropped during training\n",
    "\n",
    "Typical: 0.2 â€“ 0.5\n",
    "\n",
    "#### 12. Early stopping patience\n",
    "\n",
    "Stops training when validation loss stops improving\n",
    "\n",
    "Patience value: 5â€“10 epochs\n",
    "\n",
    "### 4. Data-related hyperparameters\n",
    "#### 13. Trainâ€“validation split\n",
    "\n",
    "Example: 80/20, 70/30\n",
    "\n",
    "#### 14. Data augmentation settings (if used)\n",
    "\n",
    "Image flips, rotations, noise, etc.\n",
    "\n",
    "Summary table (quick revision)\n",
    "Category\tHyperparameters\n",
    "Training\tLearning rate, Batch size, Epochs, Optimizer\n",
    "Architecture\tLayers, Neurons, Activation, Initialization\n",
    "Regularization\tDropout, L1/L2, Early stopping\n",
    "Data\tSplit ratio, Augmentation\n",
    "What is NOT a hyperparameter (common confusion)\n",
    "\n",
    "Weights\n",
    "\n",
    "Biases\n",
    "\n",
    "Gradients\n",
    "\n",
    "Embeddings (they are learned parameters)\n",
    "\n",
    "Typical default setup (real-world)\n",
    "\n",
    "For a beginner or standard DL project:\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Learning rate: 0.001\n",
    "\n",
    "Batch size: 32 or 64\n",
    "\n",
    "Activation (hidden): ReLU\n",
    "\n",
    "Epochs: 20â€“50\n",
    "\n",
    "Dropout: 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8a5c5-f6ed-4eff-a248-91562dcbd8b3",
   "metadata": {},
   "source": [
    "## Methods to overcome Overfitting in a Neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18aaf0-de4d-477d-9c6c-3dd1d72a07e5",
   "metadata": {},
   "source": [
    "| Method           | Sub-types                               |\n",
    "| ---------------- | --------------------------------------- |\n",
    "| Regularization   | L1, L2, Elastic Net                     |\n",
    "| Dropout          | Standard, Spatial, DropConnect          |\n",
    "| Training Control | Early stopping                          |\n",
    "| Data-based       | Augmentation, Noise injection           |\n",
    "| Architecture     | Model simplification, Parameter sharing |\n",
    "| Normalization    | Batch Normalization                     |\n",
    "| Ensemble         | Bagging, Averaging, Snapshot            |\n",
    "| Optimization     | LR scheduling, Proper splits            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd7457-b28d-4b00-97a4-8c4f90302d42",
   "metadata": {},
   "source": [
    "## 1. Regularization Methods\n",
    "\n",
    "Regularization adds constraints or penalties to prevent the model from becoming too complex.\n",
    "\n",
    "### 1.1 L1 Regularization (Lasso)\n",
    "\n",
    "Adds absolute value of weights to the loss function\n",
    "\n",
    "\n",
    "Loss=Original Loss+Î»âˆ‘âˆ£wâˆ£\n",
    "\n",
    "Encourages sparse weights (many weights become exactly 0)\n",
    "\n",
    "Performs implicit feature selection\n",
    "\n",
    "Useful when many input features are irrelevant\n",
    "\n",
    "Effect: Simpler model, reduced overfitting\n",
    "\n",
    "### 1.2 L2 Regularization (Ridge / Weight Decay)\n",
    "\n",
    "Adds square of weights to the loss function\n",
    "\n",
    "Loss=Original Loss+Î»âˆ‘w^2\n",
    "\n",
    "Penalizes large weights but does not make them zero\n",
    "\n",
    "Most commonly used in neural networks\n",
    "\n",
    "Effect: Smooths the model, prevents extreme weight values\n",
    "\n",
    "### 1.3 Elastic Net\n",
    "\n",
    "Combination of L1 + L2\n",
    "\n",
    "Controls sparsity and stability together\n",
    "\n",
    "ðŸ“Œ Used when L1 or L2 alone is insufficient\n",
    "\n",
    "## 2. Dropout\n",
    "\n",
    "Dropout randomly disables neurons during training.\n",
    "\n",
    "### 2.1 Standard Dropout\n",
    "\n",
    "Randomly sets neuron outputs to 0 with probability p\n",
    "\n",
    "Forces the network to not depend on specific neurons\n",
    "\n",
    " Common values:\n",
    "\n",
    "Fully connected layers: p = 0.5\n",
    "\n",
    "CNN layers: p = 0.2â€“0.3\n",
    "\n",
    "### 2.2 Spatial Dropout (for CNNs)\n",
    "\n",
    "Drops entire feature maps, not individual neurons\n",
    "\n",
    "Preserves spatial structure\n",
    "\n",
    "ðŸ“Œ Used mainly in convolutional layers\n",
    "\n",
    "### 2.3 DropConnect\n",
    "\n",
    "Randomly drops connections (weights) instead of neurons\n",
    "\n",
    "More computationally expensive\n",
    "\n",
    "## 3. Early Stopping\n",
    "\n",
    "Training is stopped before overfitting begins.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Monitor validation loss\n",
    "\n",
    "Stop training when validation loss starts increasing\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple\n",
    "\n",
    "No change to model architecture\n",
    "\n",
    "Very effective in practice\n",
    "\n",
    "## 4. Data-Level Techniques\n",
    "\n",
    "Overfitting often occurs due to insufficient or poor-quality data.\n",
    "\n",
    "### 4.1 Data Augmentation\n",
    "\n",
    "Artificially increases dataset size.\n",
    "\n",
    "For images:\n",
    "\n",
    "Rotation\n",
    "\n",
    "Flipping\n",
    "\n",
    "Zooming\n",
    "\n",
    "Cropping\n",
    "\n",
    "Brightness changes\n",
    "\n",
    "For text:\n",
    "\n",
    "Synonym replacement\n",
    "\n",
    "Random word deletion\n",
    "\n",
    "Back translation\n",
    "\n",
    " Improves generalization without new data collection\n",
    "\n",
    "### 4.2 Noise Injection\n",
    "\n",
    "Add small random noise to:\n",
    "\n",
    "Inputs\n",
    "\n",
    "Weights\n",
    "\n",
    "Activations\n",
    "\n",
    " Makes the model robust to variations\n",
    "\n",
    "## 5. Model Architecture Control\n",
    "\n",
    "Reduce the capacity of the model.\n",
    "\n",
    "### 5.1 Reducing Model Complexity\n",
    "\n",
    "Fewer layers\n",
    "\n",
    "Fewer neurons per layer\n",
    "\n",
    "Smaller kernel sizes (CNN)\n",
    "\n",
    " Simple models generalize better with limited data\n",
    "\n",
    "### 5.2 Parameter Sharing\n",
    "\n",
    "Used in CNNs and RNNs\n",
    "\n",
    "Same weights reused across inputs\n",
    "\n",
    "Reduces total parameters â†’ less overfitting\n",
    "\n",
    "## 6. Batch Normalization\n",
    "\n",
    "Normalizes layer inputs during training.\n",
    "\n",
    "Why it helps:\n",
    "\n",
    "Reduces internal covariate shift\n",
    "\n",
    "Adds slight regularization effect\n",
    "\n",
    "Allows higher learning rates\n",
    "\n",
    " Often reduces need for dropout\n",
    "\n",
    "## 7. Ensemble Methods\n",
    "\n",
    "Combine multiple models to reduce variance.\n",
    "\n",
    "### 7.1 Bagging\n",
    "\n",
    "Train multiple models on different subsets\n",
    "\n",
    "Average predictions\n",
    "\n",
    "### 7.2 Model Averaging\n",
    "\n",
    "Train same model multiple times with different initializations\n",
    "\n",
    "### 7.3 Snapshot Ensembles\n",
    "\n",
    "Save multiple model states from one training run\n",
    "\n",
    "Improves generalization but increases inference cost\n",
    "\n",
    "## 8. Optimization & Training Strategies\n",
    "\n",
    "Bad training setup can cause overfitting.\n",
    "\n",
    "### 8.1 Learning Rate Scheduling\n",
    "\n",
    "Reduce learning rate over time\n",
    "\n",
    "Prevents overfitting in later epochs\n",
    "\n",
    "### 8.2 Proper Trainâ€“Validation Split\n",
    "\n",
    "Ensure validation data is never seen during training\n",
    "\n",
    "Avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e9bc2-91c1-4c84-b5c5-1d8ad0000ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
